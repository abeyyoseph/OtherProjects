{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6df101f-0315-4c62-9bad-a4b9d594821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abey/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c75a8706-6734-44f4-8e7a-8fc91cddabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"OtherTrain/MOT20-02\"\n",
    "validation_dir = \"MOT20Dataset/validation/MOT20-01\"\n",
    "test_dir = \"MOT20Dataset/test/MOT20-07\"\n",
    "output_dir = \"MOT20Dataset/updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e297d98f-5fd1-4009-aad7-fd59d7126e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split MOT20-02 train set into a train and validation set\n",
    "def TrainSplit(train_directory, output_directory):\n",
    "    # Paths to dataset\n",
    "    image_dir = os.path.join(train_directory, \"images\")\n",
    "    label_dir = os.path.join(train_directory, \"labels\")\n",
    "    \n",
    "    # Create train/val directories\n",
    "    train_image_dir = os.path.join(output_directory, \"train/images\")\n",
    "    train_label_dir = os.path.join(output_directory, \"train/labels\")\n",
    "    val_image_dir = os.path.join(output_directory, \"val/images\")\n",
    "    val_label_dir = os.path.join(output_directory, \"val/labels\")\n",
    "    \n",
    "    os.makedirs(train_image_dir, exist_ok=True)\n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(val_image_dir, exist_ok=True)\n",
    "    os.makedirs(val_label_dir, exist_ok=True)\n",
    "    \n",
    "    # Get sorted list of files\n",
    "    images = sorted(os.listdir(image_dir))\n",
    "    labels = sorted(os.listdir(label_dir))\n",
    "    \n",
    "    # Ensure images and labels align\n",
    "    assert len(images) == len(labels), \"Mismatch between images and labels!\"\n",
    "    \n",
    "    # Split dataset\n",
    "    split_ratio = 0.8\n",
    "    train_count = int(len(images) * split_ratio)\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        if i < train_count:\n",
    "            shutil.move(os.path.join(image_dir, image), train_image_dir)\n",
    "            shutil.move(os.path.join(label_dir, label), train_label_dir)\n",
    "        else:\n",
    "            shutil.move(os.path.join(image_dir, image), val_image_dir)\n",
    "            shutil.move(os.path.join(label_dir, label), val_label_dir)\n",
    "    \n",
    "    print(\"Dataset splitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0375c155-115d-4772-9d41-7ef697b4e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert MOT20 dataset format to YOLO format\n",
    "def ConvertMotToYOLO(input_dir, is_test_data=False):\n",
    "    input_file = f'{input_dir}/gt/gt.txt'\n",
    "    output_dir = f'{input_dir}/labels'  # Directory to store YOLO annotations\n",
    "    image_width = 1920     \n",
    "    image_height = 1080\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process gt.txt\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split line and filter for required values\n",
    "            split_gt_line = line.strip().split(\",\")\n",
    "            \n",
    "            # Ensure we have at least the first 7 values\n",
    "            if len(split_gt_line) < 6:\n",
    "                continue  # Skip invalid lines\n",
    "            \n",
    "            # Parse required values\n",
    "            frame = int(split_gt_line[0])\n",
    "            obj_id = 0 # set every tracked pedestrian to class 0\n",
    "            bb_left = float(split_gt_line[2])\n",
    "            bb_top = float(split_gt_line[3])\n",
    "            bb_width = float(split_gt_line[4])\n",
    "            bb_height = float(split_gt_line[5])\n",
    "            # Check the provided confidence score if this is a training set\n",
    "            if is_test_data == False:\n",
    "                conf = float(split_gt_line[6])\n",
    "                \n",
    "                # Skip entries with conf == 0\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "    \n",
    "            # Normalize bounding box coordinates\n",
    "            x_center = (bb_left + bb_width / 2) / image_width\n",
    "            y_center = (bb_top + bb_height / 2) / image_height\n",
    "            width = bb_width / image_width\n",
    "            height = bb_height / image_height\n",
    "    \n",
    "            # YOLO format: <class_id> <x_center> <y_center> <width> <height>\n",
    "            yolo_line = f\"{int(obj_id)} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "    \n",
    "            # Write to frame-specific file\n",
    "            output_file = os.path.join(output_dir, f\"{int(frame):06d}.txt\")\n",
    "            with open(output_file, \"a\") as out_f:\n",
    "                out_f.write(yolo_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c90a1c2-78e9-41b4-a5fa-a635edb0ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvertMotToYOLO(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "512e3f75-011b-4e92-8a52-8a16ad285961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splitting complete!\n"
     ]
    }
   ],
   "source": [
    "TrainSplit(train_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fb1c2f-3ea9-4205-8d1a-6d28c8f1cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")  # load yolov8 nano pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26236703-df42-4ad1-8bb2-c8efd409042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.51 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.48 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i9-13900KF)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=1280, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1217 16:15:42.384740088 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/abey/Desktop/Repos/OtherProjects/ObjectTrackingMOT20/MOT20\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/abey/Desktop/Repos/OtherProjects/ObjectTrackingMOT20/MOT20Da\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.434      1.145       1.27        186       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.885      0.781      0.881      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.259     0.7951      1.189         71       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.908      0.813      0.907      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.218     0.7351      1.164        105       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.921      0.826      0.908      0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.172     0.6899      1.143        269       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.929      0.839      0.918      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.139       0.66      1.121         39       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220       0.93      0.845      0.922      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 1.849 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.48 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i9-13900KF)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220       0.92      0.834      0.917      0.615\n",
      "Speed: 1.4ms preprocess, 226.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    data='data.yaml',  # Path to your dataset YAML file\n",
    "    epochs=5,\n",
    "    batch=16,\n",
    "    imgsz=1280,\n",
    "    device='cpu',\n",
    "    freeze=10,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    weight_decay=0.001,\n",
    "    augment=True,\n",
    ")\n",
    "\n",
    "# Save the model after training\n",
    "model.save('model_5epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c849f7f-abca-482d-8156-c6d8057a2175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
