{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6df101f-0315-4c62-9bad-a4b9d594821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import csv\n",
    "import motmetrics as mm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75a8706-6734-44f4-8e7a-8fc91cddabb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"OtherTrain/MOT20-02\"\n",
    "validation_dir = \"MOT20Dataset/validation/MOT20-01\"\n",
    "test_dir = \"MOT20Dataset/test/MOT20-07\"\n",
    "output_dir = \"MOT20Dataset/updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e297d98f-5fd1-4009-aad7-fd59d7126e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split a MOT20 training set into a train and validation set\n",
    "def TrainSplit(train_directory, output_directory):\n",
    "    # Paths to dataset\n",
    "    image_dir = os.path.join(train_directory, \"images\")\n",
    "    label_dir = os.path.join(train_directory, \"labels\")\n",
    "    \n",
    "    # Create train/val directories\n",
    "    train_image_dir = os.path.join(output_directory, \"train/images\")\n",
    "    train_label_dir = os.path.join(output_directory, \"train/labels\")\n",
    "    val_image_dir = os.path.join(output_directory, \"val/images\")\n",
    "    val_label_dir = os.path.join(output_directory, \"val/labels\")\n",
    "    \n",
    "    os.makedirs(train_image_dir, exist_ok=True)\n",
    "    os.makedirs(train_label_dir, exist_ok=True)\n",
    "    os.makedirs(val_image_dir, exist_ok=True)\n",
    "    os.makedirs(val_label_dir, exist_ok=True)\n",
    "    \n",
    "    # Get sorted list of files\n",
    "    images = sorted(os.listdir(image_dir))\n",
    "    labels = sorted(os.listdir(label_dir))\n",
    "    \n",
    "    # Ensure images and labels align\n",
    "    assert len(images) == len(labels), \"Mismatch between images and labels!\"\n",
    "    \n",
    "    # Split dataset\n",
    "    split_ratio = 0.8\n",
    "    train_count = int(len(images) * split_ratio)\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        if i < train_count:\n",
    "            shutil.move(os.path.join(image_dir, image), train_image_dir)\n",
    "            shutil.move(os.path.join(label_dir, label), train_label_dir)\n",
    "        else:\n",
    "            shutil.move(os.path.join(image_dir, image), val_image_dir)\n",
    "            shutil.move(os.path.join(label_dir, label), val_label_dir)\n",
    "    \n",
    "    print(\"Dataset splitting complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0375c155-115d-4772-9d41-7ef697b4e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert MOT20 dataset format to YOLO format\n",
    "def ConvertMotToYOLO(input_dir, is_test_data=False):\n",
    "    input_file = f'{input_dir}/gt/gt.txt'\n",
    "    output_dir = f'{input_dir}/labels'  # Directory to store YOLO annotations\n",
    "    image_width = 1920     \n",
    "    image_height = 1080\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Process gt.txt\n",
    "    with open(input_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split line and filter for required values\n",
    "            split_gt_line = line.strip().split(\",\")\n",
    "            \n",
    "            # Ensure we have at least the first 7 values\n",
    "            if len(split_gt_line) < 6:\n",
    "                continue  # Skip invalid lines\n",
    "            \n",
    "            # Parse required values\n",
    "            frame = int(split_gt_line[0])\n",
    "            obj_id = 0 # set every tracked pedestrian to class 0\n",
    "            bb_left = float(split_gt_line[2])\n",
    "            bb_top = float(split_gt_line[3])\n",
    "            bb_width = float(split_gt_line[4])\n",
    "            bb_height = float(split_gt_line[5])\n",
    "            # Check the provided confidence score if this is a training set\n",
    "            if is_test_data == False:\n",
    "                conf = float(split_gt_line[6])\n",
    "                \n",
    "                # Skip entries with conf == 0\n",
    "                if conf == 0:\n",
    "                    continue\n",
    "    \n",
    "            # Normalize bounding box coordinates\n",
    "            x_center = (bb_left + bb_width / 2) / image_width\n",
    "            y_center = (bb_top + bb_height / 2) / image_height\n",
    "            width = bb_width / image_width\n",
    "            height = bb_height / image_height\n",
    "    \n",
    "            # YOLO format: <class_id> <x_center> <y_center> <width> <height>\n",
    "            yolo_line = f\"{int(obj_id)} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\"\n",
    "    \n",
    "            # Write to frame-specific file\n",
    "            output_file = os.path.join(output_dir, f\"{int(frame):06d}.txt\")\n",
    "            with open(output_file, \"a\") as out_f:\n",
    "                out_f.write(yolo_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c90a1c2-78e9-41b4-a5fa-a635edb0ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvertMotToYOLO(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "512e3f75-011b-4e92-8a52-8a16ad285961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splitting complete!\n"
     ]
    }
   ],
   "source": [
    "TrainSplit(train_dir, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34fb1c2f-3ea9-4205-8d1a-6d28c8f1cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolov8n.pt\")  # load yolov8 nano pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26236703-df42-4ad1-8bb2-c8efd409042a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.51 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.48 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i9-13900KF)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=5, time=None, patience=100, batch=16, imgsz=1280, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=10, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.001, lrf=0.01, momentum=0.937, weight_decay=0.001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1217 16:15:42.384740088 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/abey/Desktop/Repos/OtherProjects/ObjectTrackingMOT20/MOT20\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/abey/Desktop/Repos/OtherProjects/ObjectTrackingMOT20/MOT20Da\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.001), 63 bias(decay=0.0)\n",
      "Image sizes 1280 train, 1280 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.434      1.145       1.27        186       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.885      0.781      0.881      0.541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.259     0.7951      1.189         71       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.908      0.813      0.907      0.584\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G      1.218     0.7351      1.164        105       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.921      0.826      0.908      0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.172     0.6899      1.143        269       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220      0.929      0.839      0.918      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.139       0.66      1.121         39       1280: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220       0.93      0.845      0.922      0.618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 1.849 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.48 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i9-13900KF)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        557      26220       0.92      0.834      0.917      0.615\n",
      "Speed: 1.4ms preprocess, 226.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    data='data.yaml',  # Path to your dataset YAML file\n",
    "    epochs=5,\n",
    "    batch=16,\n",
    "    imgsz=1280,\n",
    "    device='cpu',\n",
    "    freeze=10,\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    weight_decay=0.001,\n",
    "    augment=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c849f7f-abca-482d-8156-c6d8057a2175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "YOLO(\n",
       "  (model): DetectionModel(\n",
       "    (model): Sequential(\n",
       "      (0): Conv(\n",
       "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv(\n",
       "        (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (2): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(48, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): Conv(\n",
       "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (4): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): Conv(\n",
       "        (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (6): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0-1): 2 x Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): Conv(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (8): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): SPPF(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
       "      )\n",
       "      (10): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (11): Concat()\n",
       "      (12): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (14): Concat()\n",
       "      (15): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): Conv(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (17): Concat()\n",
       "      (18): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (19): Conv(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "        (act): SiLU(inplace=True)\n",
       "      )\n",
       "      (20): Concat()\n",
       "      (21): C2f(\n",
       "        (cv1): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (cv2): Conv(\n",
       "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "        (m): ModuleList(\n",
       "          (0): Bottleneck(\n",
       "            (cv1): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (cv2): Conv(\n",
       "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (22): Detect(\n",
       "        (cv2): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (cv3): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(64, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(128, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv(\n",
       "              (conv): Conv2d(256, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (1): Conv(\n",
       "              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
       "              (act): SiLU(inplace=True)\n",
       "            )\n",
       "            (2): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "        )\n",
       "        (dfl): DFL(\n",
       "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(\"runs/detect/train/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c848cb4b-44f8-4b97-9ce7-bd2568531d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.48 ðŸš€ Python-3.10.12 torch-2.5.1+cu124 CPU (13th Gen Intel Core(TM) i9-13900KF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/abey/Desktop/Repos/OtherProjects/ObjectTrackingMOT20/MOT20Da\u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        585      20330      0.749      0.457      0.615      0.393\n",
      "                person        585      20330      0.749      0.457      0.615      0.393\n",
      "Speed: 2.0ms preprocess, 165.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Saving runs/detect/val5/predictions.json...\n",
      "Results saved to \u001b[1mruns/detect/val5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "results = model.val(\n",
    "    data='data.yaml',         \n",
    "    imgsz=1280,               \n",
    "    split='test',             # Use the 'test' split\n",
    "    save_json=True,           \n",
    "    save_txt=True,            \n",
    "    max_det=100,\n",
    "    conf=0.3, # Min confidence threshold for detections\n",
    "    iou=0.6) # IoU threshold for non-max suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "875a7851-5c4f-4afe-9fbc-63977572454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test images to a video for tracking\n",
    "def ConvertTestImagesToVideo(test_image_folder, output_video):\n",
    "\n",
    "    # Get sorted list of image files\n",
    "    image_files = sorted(os.listdir(image_folder), key=lambda x: int(x.split('.')[0]))\n",
    "    \n",
    "    # Read the first image to get the dimensions for the video\n",
    "    first_image_path = os.path.join(image_folder, image_files[0])\n",
    "    frame = cv2.imread(first_image_path)\n",
    "    height, width, _ = frame.shape\n",
    "    \n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can change this codec\n",
    "    video_writer = cv2.VideoWriter(output_video_path, fourcc, 30.0, (width, height))  # 30.0 is the frame rate\n",
    "    \n",
    "    # Loop through the images and write each to the video file\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(image_folder, image_file)\n",
    "        frame = cv2.imread(image_path)\n",
    "        video_writer.write(frame)\n",
    "    \n",
    "    # Release the VideoWriter\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3596c66-32c1-4eca-a34c-fdb0abec1f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to Results/MOT20-01_video.avi\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing images\n",
    "image_folder = \"MOT20Dataset/test/MOT20-01/images\"\n",
    "output_video_path = \"Results/MOT20-01_video.avi\"\n",
    "ConvertTestImagesToVideo(image_folder, output_video_path)\n",
    "print(f\"Video saved to {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d89b0cd-7dd1-4ec3-aa61-078c805f1d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ByteTrack tracker and annotators\n",
    "tracker = sv.ByteTrack()\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71a35240-b579-42d8-8f32-371c6e1effee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 173.4ms\n",
      "Speed: 18.1ms preprocess, 173.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 144.4ms\n",
      "Speed: 12.6ms preprocess, 144.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 63.3ms\n",
      "Speed: 1.5ms preprocess, 63.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 74.6ms\n",
      "Speed: 1.7ms preprocess, 74.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 102.6ms\n",
      "Speed: 23.1ms preprocess, 102.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 94.8ms\n",
      "Speed: 16.1ms preprocess, 94.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 horse, 1 umbrella, 59.8ms\n",
      "Speed: 1.5ms preprocess, 59.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 horse, 1 umbrella, 67.2ms\n",
      "Speed: 1.3ms preprocess, 67.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 108.8ms\n",
      "Speed: 1.6ms preprocess, 108.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 57.7ms\n",
      "Speed: 1.6ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 52.4ms\n",
      "Speed: 2.4ms preprocess, 52.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 1 handbag, 68.7ms\n",
      "Speed: 1.3ms preprocess, 68.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 74.5ms\n",
      "Speed: 2.2ms preprocess, 74.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 51.2ms\n",
      "Speed: 1.6ms preprocess, 51.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 79.9ms\n",
      "Speed: 1.4ms preprocess, 79.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 27.8ms\n",
      "Speed: 1.2ms preprocess, 27.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 handbag, 63.2ms\n",
      "Speed: 2.2ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 1 handbag, 79.7ms\n",
      "Speed: 1.6ms preprocess, 79.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 86.1ms\n",
      "Speed: 1.5ms preprocess, 86.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 72.1ms\n",
      "Speed: 4.9ms preprocess, 72.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 96.0ms\n",
      "Speed: 14.0ms preprocess, 96.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 61.4ms\n",
      "Speed: 2.1ms preprocess, 61.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 81.1ms\n",
      "Speed: 2.1ms preprocess, 81.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 92.2ms\n",
      "Speed: 1.5ms preprocess, 92.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 100.4ms\n",
      "Speed: 1.8ms preprocess, 100.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 189.2ms\n",
      "Speed: 1.6ms preprocess, 189.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 168.7ms\n",
      "Speed: 6.4ms preprocess, 168.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 71.1ms\n",
      "Speed: 1.7ms preprocess, 71.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 122.3ms\n",
      "Speed: 2.1ms preprocess, 122.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 100.3ms\n",
      "Speed: 7.7ms preprocess, 100.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 75.4ms\n",
      "Speed: 3.0ms preprocess, 75.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 241.4ms\n",
      "Speed: 1.6ms preprocess, 241.4ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 204.9ms\n",
      "Speed: 18.0ms preprocess, 204.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 25.4ms\n",
      "Speed: 2.1ms preprocess, 25.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 94.7ms\n",
      "Speed: 2.4ms preprocess, 94.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 118.4ms\n",
      "Speed: 5.1ms preprocess, 118.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 84.2ms\n",
      "Speed: 4.4ms preprocess, 84.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 39.1ms\n",
      "Speed: 1.4ms preprocess, 39.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 43.2ms\n",
      "Speed: 1.9ms preprocess, 43.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 1 handbag, 45.4ms\n",
      "Speed: 1.3ms preprocess, 45.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 66.0ms\n",
      "Speed: 1.8ms preprocess, 66.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 52.5ms\n",
      "Speed: 22.6ms preprocess, 52.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 2 handbags, 51.1ms\n",
      "Speed: 4.9ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 45.0ms\n",
      "Speed: 1.8ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 48.8ms\n",
      "Speed: 1.3ms preprocess, 48.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 65.1ms\n",
      "Speed: 1.9ms preprocess, 65.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 43.1ms\n",
      "Speed: 1.4ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 70.0ms\n",
      "Speed: 1.5ms preprocess, 70.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 44.5ms\n",
      "Speed: 1.9ms preprocess, 44.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 40.4ms\n",
      "Speed: 1.6ms preprocess, 40.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 52.9ms\n",
      "Speed: 1.6ms preprocess, 52.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 92.3ms\n",
      "Speed: 2.0ms preprocess, 92.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 57.1ms\n",
      "Speed: 1.3ms preprocess, 57.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 41.0ms\n",
      "Speed: 1.5ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 45.2ms\n",
      "Speed: 1.4ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 42.2ms\n",
      "Speed: 1.5ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 43.1ms\n",
      "Speed: 1.6ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 44.0ms\n",
      "Speed: 1.4ms preprocess, 44.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 57.3ms\n",
      "Speed: 1.8ms preprocess, 57.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 42.9ms\n",
      "Speed: 1.6ms preprocess, 42.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 62.5ms\n",
      "Speed: 2.3ms preprocess, 62.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 39.4ms\n",
      "Speed: 2.4ms preprocess, 39.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 37.2ms\n",
      "Speed: 1.6ms preprocess, 37.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 54.7ms\n",
      "Speed: 2.1ms preprocess, 54.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 42.0ms\n",
      "Speed: 1.5ms preprocess, 42.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 52.8ms\n",
      "Speed: 1.3ms preprocess, 52.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 40.1ms\n",
      "Speed: 1.3ms preprocess, 40.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 tv, 50.9ms\n",
      "Speed: 3.3ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 2 tvs, 48.4ms\n",
      "Speed: 1.3ms preprocess, 48.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 tv, 45.6ms\n",
      "Speed: 1.3ms preprocess, 45.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1 tv, 30.0ms\n",
      "Speed: 1.8ms preprocess, 30.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 tv, 45.3ms\n",
      "Speed: 1.7ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 tv, 44.7ms\n",
      "Speed: 1.3ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 tv, 47.7ms\n",
      "Speed: 1.7ms preprocess, 47.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 tv, 47.8ms\n",
      "Speed: 1.4ms preprocess, 47.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 tv, 50.9ms\n",
      "Speed: 1.2ms preprocess, 50.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1 tv, 45.7ms\n",
      "Speed: 1.4ms preprocess, 45.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 handbags, 1 tv, 39.5ms\n",
      "Speed: 1.9ms preprocess, 39.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 tv, 1 clock, 43.0ms\n",
      "Speed: 1.9ms preprocess, 43.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 2 tvs, 1 clock, 61.2ms\n",
      "Speed: 10.5ms preprocess, 61.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 1 tv, 53.6ms\n",
      "Speed: 1.3ms preprocess, 53.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 handbag, 1 tv, 85.9ms\n",
      "Speed: 1.9ms preprocess, 85.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 54.8ms\n",
      "Speed: 1.3ms preprocess, 54.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 61.0ms\n",
      "Speed: 1.4ms preprocess, 61.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 48.8ms\n",
      "Speed: 2.4ms preprocess, 48.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 60.1ms\n",
      "Speed: 6.8ms preprocess, 60.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 67.7ms\n",
      "Speed: 1.9ms preprocess, 67.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 46.3ms\n",
      "Speed: 1.4ms preprocess, 46.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 75.3ms\n",
      "Speed: 2.0ms preprocess, 75.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 54.4ms\n",
      "Speed: 1.3ms preprocess, 54.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 51.7ms\n",
      "Speed: 1.2ms preprocess, 51.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1 tv, 145.3ms\n",
      "Speed: 1.3ms preprocess, 145.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 tv, 89.0ms\n",
      "Speed: 1.3ms preprocess, 89.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 2 tvs, 47.0ms\n",
      "Speed: 1.3ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 tv, 56.3ms\n",
      "Speed: 1.5ms preprocess, 56.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 2 tvs, 38.9ms\n",
      "Speed: 1.4ms preprocess, 38.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 4 handbags, 2 tvs, 52.3ms\n",
      "Speed: 1.3ms preprocess, 52.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 4 handbags, 2 tvs, 45.0ms\n",
      "Speed: 1.4ms preprocess, 45.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 2 handbags, 2 tvs, 98.8ms\n",
      "Speed: 5.4ms preprocess, 98.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 1 tv, 73.8ms\n",
      "Speed: 1.2ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 12 persons, 1 handbag, 1 tv, 59.6ms\n",
      "Speed: 1.9ms preprocess, 59.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 handbag, 52.9ms\n",
      "Speed: 1.4ms preprocess, 52.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 1 tv, 66.5ms\n",
      "Speed: 1.4ms preprocess, 66.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 2 handbags, 1 tv, 112.5ms\n",
      "Speed: 1.9ms preprocess, 112.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1 tv, 67.4ms\n",
      "Speed: 1.3ms preprocess, 67.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 89.5ms\n",
      "Speed: 1.3ms preprocess, 89.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 handbag, 1 tv, 45.2ms\n",
      "Speed: 1.7ms preprocess, 45.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 1 tv, 51.1ms\n",
      "Speed: 1.4ms preprocess, 51.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 tv, 95.2ms\n",
      "Speed: 1.3ms preprocess, 95.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 2 tvs, 146.7ms\n",
      "Speed: 3.0ms preprocess, 146.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 2 handbags, 2 tvs, 60.4ms\n",
      "Speed: 1.4ms preprocess, 60.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 1 tv, 35.4ms\n",
      "Speed: 1.2ms preprocess, 35.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 43.9ms\n",
      "Speed: 1.6ms preprocess, 43.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 backpack, 1 handbag, 1 tv, 39.9ms\n",
      "Speed: 1.2ms preprocess, 39.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 backpack, 1 handbag, 1 tv, 201.6ms\n",
      "Speed: 2.1ms preprocess, 201.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 backpack, 1 handbag, 1 tv, 41.0ms\n",
      "Speed: 3.8ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 backpack, 1 handbag, 1 tv, 36.0ms\n",
      "Speed: 1.3ms preprocess, 36.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 1 tv, 32.4ms\n",
      "Speed: 6.8ms preprocess, 32.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 handbag, 2 tvs, 39.6ms\n",
      "Speed: 1.2ms preprocess, 39.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 2 handbags, 2 tvs, 36.7ms\n",
      "Speed: 1.3ms preprocess, 36.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 2 handbags, 54.0ms\n",
      "Speed: 1.4ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 handbags, 45.3ms\n",
      "Speed: 1.2ms preprocess, 45.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 handbags, 2 tvs, 41.0ms\n",
      "Speed: 1.3ms preprocess, 41.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 4 handbags, 2 tvs, 42.4ms\n",
      "Speed: 1.3ms preprocess, 42.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 handbag, 1 tv, 40.5ms\n",
      "Speed: 1.4ms preprocess, 40.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 handbag, 2 tvs, 39.5ms\n",
      "Speed: 1.4ms preprocess, 39.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 3 tvs, 190.5ms\n",
      "Speed: 4.4ms preprocess, 190.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 2 tvs, 33.0ms\n",
      "Speed: 1.5ms preprocess, 33.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 40.8ms\n",
      "Speed: 2.0ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 44.3ms\n",
      "Speed: 2.0ms preprocess, 44.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 54.7ms\n",
      "Speed: 1.3ms preprocess, 54.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 handbag, 1 tv, 41.3ms\n",
      "Speed: 1.7ms preprocess, 41.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 41.1ms\n",
      "Speed: 1.2ms preprocess, 41.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 1 tv, 35.4ms\n",
      "Speed: 2.3ms preprocess, 35.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 37.4ms\n",
      "Speed: 1.5ms preprocess, 37.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 1 handbag, 1 tv, 88.6ms\n",
      "Speed: 1.8ms preprocess, 88.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 37.1ms\n",
      "Speed: 1.4ms preprocess, 37.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 2 handbags, 43.4ms\n",
      "Speed: 1.7ms preprocess, 43.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 2 handbags, 120.9ms\n",
      "Speed: 2.4ms preprocess, 120.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 2 handbags, 1 tv, 100.1ms\n",
      "Speed: 1.4ms preprocess, 100.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 167.8ms\n",
      "Speed: 1.5ms preprocess, 167.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 1 handbag, 224.2ms\n",
      "Speed: 20.4ms preprocess, 224.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 1 handbag, 1 tv, 109.9ms\n",
      "Speed: 1.6ms preprocess, 109.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 73.4ms\n",
      "Speed: 1.3ms preprocess, 73.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 2 handbags, 1 tv, 41.5ms\n",
      "Speed: 2.2ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 1 handbag, 1 tv, 79.4ms\n",
      "Speed: 6.4ms preprocess, 79.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 handbag, 1 tv, 65.4ms\n",
      "Speed: 8.6ms preprocess, 65.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 42.5ms\n",
      "Speed: 1.3ms preprocess, 42.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 tv, 43.1ms\n",
      "Speed: 1.3ms preprocess, 43.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 handbag, 43.1ms\n",
      "Speed: 2.0ms preprocess, 43.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 43.1ms\n",
      "Speed: 1.3ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 tv, 50.0ms\n",
      "Speed: 1.9ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 horse, 1 umbrella, 38.5ms\n",
      "Speed: 1.2ms preprocess, 38.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 36.5ms\n",
      "Speed: 1.7ms preprocess, 36.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 43.1ms\n",
      "Speed: 1.2ms preprocess, 43.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 38.4ms\n",
      "Speed: 1.2ms preprocess, 38.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 102.1ms\n",
      "Speed: 1.2ms preprocess, 102.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 41.6ms\n",
      "Speed: 1.3ms preprocess, 41.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 43.6ms\n",
      "Speed: 1.2ms preprocess, 43.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 48.3ms\n",
      "Speed: 2.0ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 39.8ms\n",
      "Speed: 1.3ms preprocess, 39.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 95.2ms\n",
      "Speed: 1.2ms preprocess, 95.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 52.2ms\n",
      "Speed: 1.3ms preprocess, 52.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 44.4ms\n",
      "Speed: 1.4ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 42.2ms\n",
      "Speed: 1.4ms preprocess, 42.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 39.9ms\n",
      "Speed: 1.4ms preprocess, 39.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 39.9ms\n",
      "Speed: 1.3ms preprocess, 39.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 41.1ms\n",
      "Speed: 1.3ms preprocess, 41.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 233.8ms\n",
      "Speed: 44.6ms preprocess, 233.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 251.8ms\n",
      "Speed: 20.1ms preprocess, 251.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 255.9ms\n",
      "Speed: 2.5ms preprocess, 255.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 73.8ms\n",
      "Speed: 1.7ms preprocess, 73.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 211.9ms\n",
      "Speed: 2.0ms preprocess, 211.9ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 78.3ms\n",
      "Speed: 12.3ms preprocess, 78.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 suitcase, 47.0ms\n",
      "Speed: 1.3ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 50.5ms\n",
      "Speed: 2.1ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 72.6ms\n",
      "Speed: 1.3ms preprocess, 72.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 59.3ms\n",
      "Speed: 1.4ms preprocess, 59.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 51.1ms\n",
      "Speed: 1.9ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 90.8ms\n",
      "Speed: 9.9ms preprocess, 90.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 57.0ms\n",
      "Speed: 1.3ms preprocess, 57.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 64.3ms\n",
      "Speed: 1.3ms preprocess, 64.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 92.7ms\n",
      "Speed: 1.4ms preprocess, 92.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 tv, 54.4ms\n",
      "Speed: 1.3ms preprocess, 54.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 1 tv, 66.1ms\n",
      "Speed: 1.3ms preprocess, 66.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 54.0ms\n",
      "Speed: 1.3ms preprocess, 54.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 72.5ms\n",
      "Speed: 1.2ms preprocess, 72.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 1 handbag, 47.3ms\n",
      "Speed: 1.4ms preprocess, 47.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 1 handbag, 44.4ms\n",
      "Speed: 1.2ms preprocess, 44.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 1 tv, 49.4ms\n",
      "Speed: 1.5ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 48.0ms\n",
      "Speed: 2.5ms preprocess, 48.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 1 tv, 51.5ms\n",
      "Speed: 1.5ms preprocess, 51.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 51.9ms\n",
      "Speed: 1.7ms preprocess, 51.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 53.3ms\n",
      "Speed: 1.7ms preprocess, 53.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 54.8ms\n",
      "Speed: 1.8ms preprocess, 54.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 2 handbags, 81.8ms\n",
      "Speed: 7.9ms preprocess, 81.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 45.2ms\n",
      "Speed: 1.8ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 47.3ms\n",
      "Speed: 1.6ms preprocess, 47.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 57.6ms\n",
      "Speed: 2.0ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 tv, 50.0ms\n",
      "Speed: 1.3ms preprocess, 50.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 130.9ms\n",
      "Speed: 1.3ms preprocess, 130.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 handbag, 2 tvs, 237.1ms\n",
      "Speed: 1.8ms preprocess, 237.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 86.1ms\n",
      "Speed: 1.5ms preprocess, 86.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 127.4ms\n",
      "Speed: 1.7ms preprocess, 127.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 68.3ms\n",
      "Speed: 3.4ms preprocess, 68.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 51.1ms\n",
      "Speed: 1.8ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 37.7ms\n",
      "Speed: 1.7ms preprocess, 37.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 backpack, 1 tv, 111.0ms\n",
      "Speed: 23.4ms preprocess, 111.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 tv, 46.7ms\n",
      "Speed: 1.4ms preprocess, 46.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 40.6ms\n",
      "Speed: 1.2ms preprocess, 40.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 42.9ms\n",
      "Speed: 1.7ms preprocess, 42.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 70.8ms\n",
      "Speed: 1.6ms preprocess, 70.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 46.2ms\n",
      "Speed: 4.3ms preprocess, 46.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 tvs, 46.7ms\n",
      "Speed: 1.3ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 tvs, 49.0ms\n",
      "Speed: 1.3ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 handbag, 2 tvs, 48.2ms\n",
      "Speed: 1.2ms preprocess, 48.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 89.1ms\n",
      "Speed: 1.7ms preprocess, 89.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 63.9ms\n",
      "Speed: 1.2ms preprocess, 63.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 tvs, 67.0ms\n",
      "Speed: 1.3ms preprocess, 67.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 79.8ms\n",
      "Speed: 1.5ms preprocess, 79.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 53.4ms\n",
      "Speed: 1.3ms preprocess, 53.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 tv, 144.0ms\n",
      "Speed: 1.2ms preprocess, 144.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 40.3ms\n",
      "Speed: 1.6ms preprocess, 40.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 tv, 42.5ms\n",
      "Speed: 1.4ms preprocess, 42.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 tv, 55.3ms\n",
      "Speed: 1.4ms preprocess, 55.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 tv, 58.5ms\n",
      "Speed: 1.9ms preprocess, 58.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 tv, 54.2ms\n",
      "Speed: 5.9ms preprocess, 54.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 tv, 38.9ms\n",
      "Speed: 1.2ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 tvs, 58.1ms\n",
      "Speed: 1.3ms preprocess, 58.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 2 handbags, 2 tvs, 44.7ms\n",
      "Speed: 1.9ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 53.4ms\n",
      "Speed: 1.3ms preprocess, 53.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 handbag, 2 tvs, 49.9ms\n",
      "Speed: 1.3ms preprocess, 49.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 55.3ms\n",
      "Speed: 1.5ms preprocess, 55.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 tvs, 47.2ms\n",
      "Speed: 1.3ms preprocess, 47.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 46.5ms\n",
      "Speed: 1.4ms preprocess, 46.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 48.7ms\n",
      "Speed: 1.2ms preprocess, 48.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 2 tvs, 61.5ms\n",
      "Speed: 1.2ms preprocess, 61.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 57.6ms\n",
      "Speed: 1.6ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 47.8ms\n",
      "Speed: 1.3ms preprocess, 47.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 tvs, 108.3ms\n",
      "Speed: 2.0ms preprocess, 108.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 tv, 76.4ms\n",
      "Speed: 12.8ms preprocess, 76.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 tv, 51.4ms\n",
      "Speed: 1.3ms preprocess, 51.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 72.3ms\n",
      "Speed: 1.5ms preprocess, 72.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 60.5ms\n",
      "Speed: 1.2ms preprocess, 60.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 64.0ms\n",
      "Speed: 1.3ms preprocess, 64.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 78.2ms\n",
      "Speed: 1.4ms preprocess, 78.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 tvs, 53.3ms\n",
      "Speed: 1.4ms preprocess, 53.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 2 tvs, 68.1ms\n",
      "Speed: 1.4ms preprocess, 68.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 2 tvs, 65.9ms\n",
      "Speed: 1.2ms preprocess, 65.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 49.7ms\n",
      "Speed: 3.8ms preprocess, 49.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 56.2ms\n",
      "Speed: 1.3ms preprocess, 56.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 82.4ms\n",
      "Speed: 2.2ms preprocess, 82.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 66.5ms\n",
      "Speed: 1.2ms preprocess, 66.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 tv, 50.6ms\n",
      "Speed: 1.7ms preprocess, 50.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 tv, 52.6ms\n",
      "Speed: 1.3ms preprocess, 52.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 tvs, 86.6ms\n",
      "Speed: 6.0ms preprocess, 86.6ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 tv, 177.1ms\n",
      "Speed: 4.9ms preprocess, 177.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 tv, 70.7ms\n",
      "Speed: 1.7ms preprocess, 70.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 tv, 75.0ms\n",
      "Speed: 2.0ms preprocess, 75.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 68.0ms\n",
      "Speed: 3.7ms preprocess, 68.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 64.4ms\n",
      "Speed: 1.5ms preprocess, 64.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 55.8ms\n",
      "Speed: 1.3ms preprocess, 55.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 54.0ms\n",
      "Speed: 1.6ms preprocess, 54.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 tv, 50.0ms\n",
      "Speed: 1.3ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 tv, 50.3ms\n",
      "Speed: 4.0ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 46.4ms\n",
      "Speed: 11.4ms preprocess, 46.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 43.0ms\n",
      "Speed: 1.3ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 42.0ms\n",
      "Speed: 1.4ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 188.7ms\n",
      "Speed: 5.3ms preprocess, 188.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 tvs, 92.9ms\n",
      "Speed: 1.4ms preprocess, 92.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 tvs, 115.5ms\n",
      "Speed: 10.4ms preprocess, 115.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 46.7ms\n",
      "Speed: 1.9ms preprocess, 46.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 tvs, 57.7ms\n",
      "Speed: 1.7ms preprocess, 57.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 42.0ms\n",
      "Speed: 1.4ms preprocess, 42.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 57.6ms\n",
      "Speed: 3.4ms preprocess, 57.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 56.8ms\n",
      "Speed: 1.9ms preprocess, 56.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 62.7ms\n",
      "Speed: 1.2ms preprocess, 62.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 64.6ms\n",
      "Speed: 1.3ms preprocess, 64.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 2 tvs, 64.9ms\n",
      "Speed: 1.4ms preprocess, 64.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 2 tvs, 52.3ms\n",
      "Speed: 1.3ms preprocess, 52.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 2 tvs, 81.0ms\n",
      "Speed: 1.6ms preprocess, 81.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 3 tvs, 92.1ms\n",
      "Speed: 1.3ms preprocess, 92.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 46.9ms\n",
      "Speed: 1.4ms preprocess, 46.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 51.1ms\n",
      "Speed: 1.4ms preprocess, 51.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 2 tvs, 48.8ms\n",
      "Speed: 1.1ms preprocess, 48.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 3 tvs, 57.6ms\n",
      "Speed: 9.3ms preprocess, 57.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 3 tvs, 71.6ms\n",
      "Speed: 1.3ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 27 persons, 3 tvs, 50.4ms\n",
      "Speed: 1.8ms preprocess, 50.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 2 backpacks, 3 tvs, 57.3ms\n",
      "Speed: 1.2ms preprocess, 57.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 44.6ms\n",
      "Speed: 1.7ms preprocess, 44.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 2 tvs, 38.9ms\n",
      "Speed: 1.3ms preprocess, 38.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 backpack, 2 tvs, 41.5ms\n",
      "Speed: 1.3ms preprocess, 41.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 2 tvs, 54.1ms\n",
      "Speed: 2.1ms preprocess, 54.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 backpack, 1 handbag, 2 tvs, 55.5ms\n",
      "Speed: 3.6ms preprocess, 55.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 backpack, 2 tvs, 77.3ms\n",
      "Speed: 1.5ms preprocess, 77.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 backpack, 2 tvs, 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 67.6ms\n",
      "Speed: 1.2ms preprocess, 67.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 handbag, 2 tvs, 37.2ms\n",
      "Speed: 1.5ms preprocess, 37.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 2 tvs, 103.4ms\n",
      "Speed: 1.3ms preprocess, 103.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 backpack, 2 tvs, 48.4ms\n",
      "Speed: 1.4ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 1 handbag, 2 tvs, 71.6ms\n",
      "Speed: 5.2ms preprocess, 71.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 umbrella, 2 tvs, 45.2ms\n",
      "Speed: 1.3ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 2 tvs, 44.1ms\n",
      "Speed: 8.0ms preprocess, 44.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 2 tvs, 50.2ms\n",
      "Speed: 1.4ms preprocess, 50.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 2 tvs, 48.9ms\n",
      "Speed: 1.8ms preprocess, 48.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 2 tvs, 63.6ms\n",
      "Speed: 1.2ms preprocess, 63.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 1 handbag, 2 tvs, 69.2ms\n",
      "Speed: 1.5ms preprocess, 69.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 2 tvs, 51.6ms\n",
      "Speed: 1.7ms preprocess, 51.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 umbrella, 2 tvs, 44.7ms\n",
      "Speed: 1.3ms preprocess, 44.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 2 tvs, 49.6ms\n",
      "Speed: 1.5ms preprocess, 49.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 2 tvs, 71.0ms\n",
      "Speed: 1.6ms preprocess, 71.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 2 tvs, 59.3ms\n",
      "Speed: 4.0ms preprocess, 59.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 53.0ms\n",
      "Speed: 1.4ms preprocess, 53.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 45.2ms\n",
      "Speed: 2.2ms preprocess, 45.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 57.8ms\n",
      "Speed: 1.2ms preprocess, 57.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 2 tvs, 63.2ms\n",
      "Speed: 1.4ms preprocess, 63.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 47.6ms\n",
      "Speed: 1.8ms preprocess, 47.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 48.9ms\n",
      "Speed: 1.3ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 62.7ms\n",
      "Speed: 1.5ms preprocess, 62.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 40.2ms\n",
      "Speed: 1.5ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 45.9ms\n",
      "Speed: 1.6ms preprocess, 45.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 47.1ms\n",
      "Speed: 1.3ms preprocess, 47.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 43.0ms\n",
      "Speed: 1.9ms preprocess, 43.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 55.3ms\n",
      "Speed: 1.2ms preprocess, 55.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 52.5ms\n",
      "Speed: 1.3ms preprocess, 52.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 60.4ms\n",
      "Speed: 1.5ms preprocess, 60.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 105.1ms\n",
      "Speed: 2.0ms preprocess, 105.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 46.4ms\n",
      "Speed: 5.7ms preprocess, 46.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 44.6ms\n",
      "Speed: 1.5ms preprocess, 44.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 43.3ms\n",
      "Speed: 1.3ms preprocess, 43.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 172.0ms\n",
      "Speed: 19.2ms preprocess, 172.0ms inference, 8.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 73.3ms\n",
      "Speed: 1.4ms preprocess, 73.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 umbrella, 47.5ms\n",
      "Speed: 1.2ms preprocess, 47.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 29 persons, 1 umbrella, 73.2ms\n",
      "Speed: 1.3ms preprocess, 73.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 umbrella, 55.9ms\n",
      "Speed: 1.3ms preprocess, 55.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 umbrella, 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 1 handbag, 69.3ms\n",
      "Speed: 1.7ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 51.3ms\n",
      "Speed: 1.5ms preprocess, 51.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 25 persons, 1 umbrella, 70.6ms\n",
      "Speed: 1.2ms preprocess, 70.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 56.9ms\n",
      "Speed: 1.5ms preprocess, 56.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 58.9ms\n",
      "Speed: 1.9ms preprocess, 58.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 51.7ms\n",
      "Speed: 2.8ms preprocess, 51.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 61.8ms\n",
      "Speed: 1.5ms preprocess, 61.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 226.5ms\n",
      "Speed: 4.6ms preprocess, 226.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 26 persons, 1 umbrella, 62.0ms\n",
      "Speed: 1.5ms preprocess, 62.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 47.5ms\n",
      "Speed: 1.4ms preprocess, 47.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 51.8ms\n",
      "Speed: 1.7ms preprocess, 51.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 36.9ms\n",
      "Speed: 1.5ms preprocess, 36.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 64.0ms\n",
      "Speed: 1.9ms preprocess, 64.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 61.6ms\n",
      "Speed: 1.4ms preprocess, 61.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 50.5ms\n",
      "Speed: 1.3ms preprocess, 50.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 47.0ms\n",
      "Speed: 1.2ms preprocess, 47.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 1 umbrella, 70.4ms\n",
      "Speed: 1.3ms preprocess, 70.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 24 persons, 1 umbrella, 47.9ms\n",
      "Speed: 1.3ms preprocess, 47.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 23 persons, 1 umbrella, 59.6ms\n",
      "Speed: 1.3ms preprocess, 59.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 22 persons, 1 umbrella, 104.1ms\n",
      "Speed: 1.4ms preprocess, 104.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 48.4ms\n",
      "Speed: 1.4ms preprocess, 48.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 55.7ms\n",
      "Speed: 1.4ms preprocess, 55.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 42.9ms\n",
      "Speed: 1.3ms preprocess, 42.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 43.9ms\n",
      "Speed: 1.4ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 65.0ms\n",
      "Speed: 2.1ms preprocess, 65.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 45.2ms\n",
      "Speed: 1.8ms preprocess, 45.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 74.2ms\n",
      "Speed: 13.0ms preprocess, 74.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 46.2ms\n",
      "Speed: 1.7ms preprocess, 46.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 128.3ms\n",
      "Speed: 29.2ms preprocess, 128.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 48.3ms\n",
      "Speed: 1.9ms preprocess, 48.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 49.0ms\n",
      "Speed: 4.4ms preprocess, 49.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 65.8ms\n",
      "Speed: 1.5ms preprocess, 65.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 49.4ms\n",
      "Speed: 1.3ms preprocess, 49.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 47.9ms\n",
      "Speed: 1.9ms preprocess, 47.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 44.0ms\n",
      "Speed: 1.3ms preprocess, 44.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 40.8ms\n",
      "Speed: 1.4ms preprocess, 40.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 43.8ms\n",
      "Speed: 1.5ms preprocess, 43.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 46.0ms\n",
      "Speed: 7.4ms preprocess, 46.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 50.0ms\n",
      "Speed: 1.6ms preprocess, 50.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 45.1ms\n",
      "Speed: 1.2ms preprocess, 45.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 47.3ms\n",
      "Speed: 3.0ms preprocess, 47.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 50.3ms\n",
      "Speed: 1.2ms preprocess, 50.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 56.6ms\n",
      "Speed: 1.3ms preprocess, 56.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 83.8ms\n",
      "Speed: 1.7ms preprocess, 83.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 31.0ms\n",
      "Speed: 1.3ms preprocess, 31.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 62.9ms\n",
      "Speed: 1.4ms preprocess, 62.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 47.3ms\n",
      "Speed: 1.8ms preprocess, 47.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 31.6ms\n",
      "Speed: 1.6ms preprocess, 31.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 31.1ms\n",
      "Speed: 2.1ms preprocess, 31.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 1 handbag, 60.7ms\n",
      "Speed: 5.8ms preprocess, 60.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 30.7ms\n",
      "Speed: 1.4ms preprocess, 30.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 30.1ms\n",
      "Speed: 1.4ms preprocess, 30.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 31.6ms\n",
      "Speed: 1.5ms preprocess, 31.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 13 persons, 1 umbrella, 61.8ms\n",
      "Speed: 6.5ms preprocess, 61.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 36.8ms\n",
      "Speed: 1.2ms preprocess, 36.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 14 persons, 1 umbrella, 2 handbags, 34.9ms\n",
      "Speed: 1.3ms preprocess, 34.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 umbrella, 2 handbags, 49.4ms\n",
      "Speed: 1.8ms preprocess, 49.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 2 handbags, 44.9ms\n",
      "Speed: 1.5ms preprocess, 44.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 1 umbrella, 38.2ms\n",
      "Speed: 8.8ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 52.0ms\n",
      "Speed: 2.3ms preprocess, 52.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 48.0ms\n",
      "Speed: 1.3ms preprocess, 48.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 43.7ms\n",
      "Speed: 1.7ms preprocess, 43.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 1 umbrella, 1 handbag, 55.0ms\n",
      "Speed: 1.6ms preprocess, 55.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 49.6ms\n",
      "Speed: 1.3ms preprocess, 49.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 43.9ms\n",
      "Speed: 1.3ms preprocess, 43.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 1 handbag, 58.3ms\n",
      "Speed: 1.4ms preprocess, 58.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 1 handbag, 38.7ms\n",
      "Speed: 1.4ms preprocess, 38.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 48.0ms\n",
      "Speed: 2.8ms preprocess, 48.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 1 handbag, 80.2ms\n",
      "Speed: 1.4ms preprocess, 80.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 60.3ms\n",
      "Speed: 1.6ms preprocess, 60.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 handbag, 56.5ms\n",
      "Speed: 1.6ms preprocess, 56.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 52.8ms\n",
      "Speed: 2.0ms preprocess, 52.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 48.9ms\n",
      "Speed: 1.6ms preprocess, 48.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 40.2ms\n",
      "Speed: 1.4ms preprocess, 40.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 27.6ms\n",
      "Speed: 2.0ms preprocess, 27.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 30.7ms\n",
      "Speed: 1.9ms preprocess, 30.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 38.2ms\n",
      "Speed: 2.6ms preprocess, 38.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 21 persons, 35.1ms\n",
      "Speed: 1.2ms preprocess, 35.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 44.2ms\n",
      "Speed: 1.2ms preprocess, 44.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 16 persons, 29.8ms\n",
      "Speed: 1.4ms preprocess, 29.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 43.8ms\n",
      "Speed: 1.3ms preprocess, 43.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 1 umbrella, 29.8ms\n",
      "Speed: 1.4ms preprocess, 29.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 20 persons, 53.8ms\n",
      "Speed: 1.8ms preprocess, 53.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 42.3ms\n",
      "Speed: 1.2ms preprocess, 42.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 45.6ms\n",
      "Speed: 1.3ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 51.9ms\n",
      "Speed: 1.8ms preprocess, 51.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1 umbrella, 50.1ms\n",
      "Speed: 1.6ms preprocess, 50.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 15 persons, 1 umbrella, 30.6ms\n",
      "Speed: 2.2ms preprocess, 30.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 17 persons, 50.0ms\n",
      "Speed: 1.3ms preprocess, 50.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 26.1ms\n",
      "Speed: 1.3ms preprocess, 26.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 18 persons, 40.3ms\n",
      "Speed: 1.3ms preprocess, 40.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 58.3ms\n",
      "Speed: 1.3ms preprocess, 58.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 19 persons, 74.1ms\n",
      "Speed: 1.3ms preprocess, 74.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# File to save tracking results\n",
    "output_file = \"Results/MOT20-01.txt\"\n",
    "\n",
    "# Open the results file for logging\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    def callback(frame: np.ndarray, frame_id: int) -> np.ndarray:\n",
    "        results = model.predict(frame)[0]\n",
    "        # Extract detections from results\n",
    "        boxes = results.boxes.xyxy.cpu().numpy()  # Bounding box coordinates\n",
    "        scores = results.boxes.conf.cpu().numpy()  # Confidence scores\n",
    "        class_ids = results.boxes.cls.cpu().numpy().astype(int)  # Class IDs\n",
    "\n",
    "        # Create Supervision detections\n",
    "        detections = sv.Detections(\n",
    "            xyxy=boxes,\n",
    "            confidence=scores,\n",
    "            class_id=class_ids\n",
    "        )\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "\n",
    "        # Log tracking results in MOTChallenge format\n",
    "        for box, score, class_id, tracker_id in zip(\n",
    "            detections.xyxy, detections.confidence, detections.class_id, detections.tracker_id\n",
    "        ):\n",
    "            x1, y1, x2, y2 = box\n",
    "            w, h = x2 - x1, y2 - y1  # Convert to width and height\n",
    "            writer.writerow([frame_id + 1, tracker_id, x1, y1, w, h, score, class_id, -1])\n",
    "\n",
    "        # Create labels with class names and tracker IDs\n",
    "        labels = [\n",
    "            f\"#{tracker_id} {results.names[class_id]}\"\n",
    "            for tracker_id, class_id in zip(detections.tracker_id, detections.class_id)\n",
    "        ]\n",
    "\n",
    "        annotated_frame = box_annotator.annotate(\n",
    "            frame.copy(), detections=detections)\n",
    "        return label_annotator.annotate(\n",
    "            annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    sv.process_video(\n",
    "        source_path=\"Results/MOT20-01_video.avi\",\n",
    "        target_path=\"Results/MOT20-01_detections.avi\",\n",
    "        callback=callback\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5afc55b3-60ff-45dc-a2c3-4e40d30916f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IoU (intersection-over-union) matrix between ground truth and predictions\n",
    "def compute_iou_matrix(gt_boxes, pred_boxes):\n",
    "    iou_matrix = np.zeros((len(gt_boxes), len(pred_boxes)))\n",
    "    for i, gt in enumerate(gt_boxes):\n",
    "        for j, pred in enumerate(pred_boxes):\n",
    "            # Calculate intersection\n",
    "            x1 = max(gt[0], pred[0])\n",
    "            y1 = max(gt[1], pred[1])\n",
    "            x2 = min(gt[2], pred[2])\n",
    "            y2 = min(gt[3], pred[3])\n",
    "            intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "\n",
    "            # Calculate union\n",
    "            gt_area = (gt[2] - gt[0]) * (gt[3] - gt[1])\n",
    "            pred_area = (pred[2] - pred[0]) * (pred[3] - pred[1])\n",
    "            union = gt_area + pred_area - intersection\n",
    "\n",
    "            # Compute IoU\n",
    "            iou_matrix[i, j] = intersection / union if union > 0 else 0\n",
    "    return iou_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4516d1c0-2df4-48c7-8103-0c7b632076a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          MOTA  IDF1   Prcn  Rcll\n",
      "MOT20-01 28.2% 44.2% 100.0% 28.3%\n"
     ]
    }
   ],
   "source": [
    "# Paths to the ground truth and tracking results\n",
    "gt_path = \"MOT20Dataset/test/MOT20-01/gt/gt.txt\"  \n",
    "results_path = \"Results/MOT20-01.txt\" \n",
    "\n",
    "gt_columns = [\"frame\", \"id\", \"x\", \"y\", \"w\", \"h\", \"_1\", \"_2\", \"_3\", \"_4\"]\n",
    "det_columns = [\"frame\", \"_\", \"x\", \"y\", \"w\", \"h\", \"confidence\", \"_1\", \"_2\", \"_3\"]\n",
    "\n",
    "# Adjust column names to match the file structure\n",
    "gt_data = pd.read_csv(gt_path, header=None)\n",
    "det_data = pd.read_csv(results_path, header=None)\n",
    "\n",
    "# Assign appropriate column names based on actual data shape\n",
    "gt_data.columns = gt_columns[:gt_data.shape[1]]\n",
    "det_data.columns = det_columns[:det_data.shape[1]]\n",
    "\n",
    "# Initialize MOTAccumulator\n",
    "acc = mm.MOTAccumulator(auto_id=True)\n",
    "\n",
    "# Process frames\n",
    "frames = sorted(gt_data[\"frame\"].unique())\n",
    "for frame_id in frames:\n",
    "    # Ground truth for this frame\n",
    "    gt_frame = gt_data[gt_data[\"frame\"] == frame_id]\n",
    "    gt_ids = gt_frame[\"id\"].values\n",
    "    gt_boxes = np.array(\n",
    "        [\n",
    "            [row[\"x\"], row[\"y\"], row[\"x\"] + row[\"w\"], row[\"y\"] + row[\"h\"]]\n",
    "            for _, row in gt_frame.iterrows()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Detections for this frame\n",
    "    det_frame = det_data[det_data[\"frame\"] == frame_id]\n",
    "    pred_ids = np.arange(len(det_frame))  # Assign unique IDs to detections\n",
    "    pred_boxes = np.array(\n",
    "        [\n",
    "            [row[\"x\"], row[\"y\"], row[\"x\"] + row[\"w\"], row[\"y\"] + row[\"h\"]]\n",
    "            for _, row in det_frame.iterrows()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compute IoU matrix\n",
    "    if len(gt_boxes) > 0 and len(pred_boxes) > 0:\n",
    "        iou_matrix = compute_iou_matrix(gt_boxes, pred_boxes)\n",
    "    else:\n",
    "        iou_matrix = np.empty((len(gt_boxes), len(pred_boxes)))\n",
    "\n",
    "    # Update accumulator\n",
    "    acc.update(gt_ids, pred_ids, iou_matrix)\n",
    "\n",
    "# Compute metrics\n",
    "mh = mm.metrics.create()\n",
    "metrics = [\"mota\", \"idf1\", \"precision\", \"recall\"]\n",
    "summary = mh.compute(acc, metrics=metrics, name=\"MOT20-01\")\n",
    "\n",
    "# Print results\n",
    "print(mm.io.render_summary(summary, formatters=mh.formatters, namemap=mm.io.motchallenge_metric_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0886b8-9f31-4432-bf22-34441277a4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39577265-069c-4377-9b61-2ecae74c0ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
