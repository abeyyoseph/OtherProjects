{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60503ee4-4cd1-4d99-860d-95decab1fce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler, Subset\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd5a935-3c6b-43e3-af3d-0d67e20a2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"Images/FloodNet Challenge - Track 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da827e40-8f69-4fd4-8d23-f487e75a12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentations and preprocessing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # ImageNet stats\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d0d496-2b0a-42da-bd23-5bfbbc4e1967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class to Index Mapping: {'Flooded': 0, 'Non-Flooded': 1}\n",
      "Class Distribution:\n",
      "Flooded: 51\n",
      "Non-Flooded: 347\n"
     ]
    }
   ],
   "source": [
    "# Load the labeled training data\n",
    "train_dataset = datasets.ImageFolder(f'{base_dir}/Train/Labeled', transform=train_transforms)\n",
    "# Get the class names and their corresponding indices\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "print(f\"Class to Index Mapping: {class_to_idx}\")\n",
    "\n",
    "# Count the samples in each class\n",
    "class_counts = {class_name: 0 for class_name in class_to_idx.keys()}\n",
    "for _, label in train_dataset.samples:\n",
    "    for class_name, class_idx in class_to_idx.items():\n",
    "        if label == class_idx:\n",
    "            class_counts[class_name] += 1\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"{class_name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf11aaab-5108-4e71-9bcd-794266b5703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5443/3797760210.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Large class imbalance so need to use WeightedRandomSampler to ensure balanced mini-batches\n",
    "# during training.\n",
    "\n",
    "# Extract class labels from the dataset\n",
    "targets = train_dataset.targets\n",
    "\n",
    "# Calculate class weights, which are the inverse of class frequencies. Classes with fewer samples will get assigned a higher \n",
    "# weight (ensuring that the minority class receives a higher weight, making it more likely to be sampled during training).\n",
    "class_counts = np.bincount(targets)  # Count the number of samples per class\n",
    "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "sample_weights = torch.tensor([class_weights[label] for label in targets], dtype=torch.float)\n",
    "\n",
    "# Create the WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders with the sampler for training\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=sampler, num_workers=4)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5791f5b-fc45-4738-abfa-ef86fcf36db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ViT model for transfer learning\n",
    "model = models.vit_b_16(weights=ViT_B_16_Weights.DEFAULT )\n",
    "# Freeze the parameters in the base model so only the new layers are being updated\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the final layer with a FC layer\n",
    "num_classes = len(class_counts)\n",
    "\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)\n",
    "\n",
    "# Unfreeze the final layer to allow it to learn during training\n",
    "for param in model.heads.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize the loss function with the class weights to penalize errors made on the minority class more heavily.\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Decrease learning rate by 0.1 if loss hasn't decreased after 3 consecutive epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.1, patience=3, verbose=True\n",
    ")\n",
    "# Set the number of epochs for training\n",
    "num_epochs = 20\n",
    "\n",
    "# Metrics tracker\n",
    "f1_metric = MulticlassF1Score(num_classes=num_classes, average='weighted').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "412c8681-1d3a-45df-a6d3-ab4cadeadb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, \n",
    "                is_initial=True, val_loader=None):\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    save_path = \"best_initial_model.pth\" if is_initial else \"best_retrained_model.pth\"\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Check if validation should be used for model saving\n",
    "        if not is_initial and val_loader is not None:\n",
    "            # Validation step\n",
    "            model.eval()\n",
    "            total_confidence = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_images in val_loader:\n",
    "                    val_images = val_images.to(device)\n",
    "                    val_outputs = model(val_images)\n",
    "                    confidences = torch.softmax(val_outputs, dim=1).max(dim=1).values  # Max confidence per sample\n",
    "                    total_confidence += confidences.sum().item()\n",
    "            \n",
    "            avg_confidence = total_confidence / len(val_loader.dataset)\n",
    "            print(f\"Average Confidence: {avg_confidence:.4f}\")\n",
    "        \n",
    "        else:\n",
    "            # Save the best model based on training loss\n",
    "            if avg_train_loss < best_loss:\n",
    "                best_loss = avg_train_loss\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"New best model saved with training loss: {best_loss:.4f}\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d15ae61-3bb3-4a1c-b090-efee0ee5ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=5,\n",
    "    device=device,\n",
    "    is_initial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cef44486-12ec-49a7-b42b-cf7815e7bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pseudo_labels(model, unlabeled_dataset, batch_size, confidence_threshold=0.9):\n",
    "    # Path to the best performing initial model\n",
    "    model_path = os.path.join(\".\", \"best_initial_model.pth\")\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    print(\"Loaded best initial model for generation of pseudo labels on unlabeled training data\")\n",
    "    \n",
    "    # DataLoader for unlabeled data\n",
    "    unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    pseudo_labeled_data = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in unlabeled_loader:  # The labels are not used for unlabeled data\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probabilities = F.softmax(outputs, dim=1)  # Get class probabilities\n",
    "            confidences, pseudo_labels = torch.max(probabilities, dim=1)  # Max confidence and corresponding class\n",
    "    \n",
    "            # Filter based on confidence threshold\n",
    "            for i in range(len(images)):\n",
    "                if confidences[i] > confidence_threshold:\n",
    "                    pseudo_labeled_data.append((images[i].cpu(), pseudo_labels[i].cpu()))\n",
    "    \n",
    "    print(f\"Pseudo-labeled {len(pseudo_labeled_data)} images from the unlabeled dataset.\")\n",
    "\n",
    "    return pseudo_labeled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d274295d-22f9-4ab5-aeeb-1ce1ac974de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabeledDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # data should be a list of (image, label) tuples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.data[index]\n",
    "        # Ensure the image is a tensor, and the label is a tensor of type torch.long\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a7c9ae5-792f-45a2-ab99-be481d79078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Ensure image is in RGB mode\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44de1d48-c1d4-4770-bf5e-68294b156fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorLabelDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        label = torch.tensor(label, dtype=torch.long)  # Convert to tensor\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3320766-0d6b-47a0-af3b-335c944ea1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best initial model for generation of pseudo labels on unlabeled training data\n",
      "Pseudo-labeled 722 images from the unlabeled dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load the unlabeled training data\n",
    "train_unlabeled_dataset = datasets.ImageFolder(f'{base_dir}/Train/Unlabeled', transform=train_transforms)\n",
    "\n",
    "# Generate the pseudo labels for the unlabeled dataset\n",
    "pseudo_labeled_data = generate_pseudo_labels(model, train_unlabeled_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27c0e495-11d5-4abf-9fe7-2debf14a601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_dataset = PseudoLabeledDataset(pseudo_labeled_data)\n",
    "\n",
    "# Need to use custom dataset that sets labels to tensors instead of ints for uniformity with the unlabeled dataset\n",
    "train_dataset = TensorLabelDataset(train_dataset)\n",
    "# Combine labeled and pseudo-labeled datasets\n",
    "augmented_dataset = ConcatDataset([train_dataset, pseudo_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab6e622b-2123-4e30-83c5-d3332c2e61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract class labels (targets) from the original ImageFolder dataset\n",
    "original_targets = train_dataset.dataset.targets  # Access the underlying dataset if it's wrapped\n",
    "\n",
    "# Convert original targets to a list (if they aren't already)\n",
    "original_targets = list(original_targets)\n",
    "\n",
    "# Extract pseudo-labels from the pseudo-labeled dataset\n",
    "pseudo_targets = [label for _, label in pseudo_labeled_data]\n",
    "\n",
    "# Combine the targets from both datasets\n",
    "all_targets = original_targets + pseudo_targets\n",
    "\n",
    "# Calculate class weights (inverse of class frequencies)\n",
    "class_counts = np.bincount(all_targets)\n",
    "class_weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
    "\n",
    "# Assign weights to each sample in the augmented dataset\n",
    "sample_weights = torch.tensor([class_weights[label] for label in all_targets], dtype=torch.float)\n",
    "\n",
    "# Create the WeightedRandomSampler\n",
    "augmented_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create the DataLoader\n",
    "augmented_loader = DataLoader(augmented_dataset, batch_size=32, sampler=augmented_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42be73a1-cd69-430e-b951-ada2e29a9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data augmentations and preprocessing for the validation dataset\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize the dataset\n",
    "val_dataset = UnlabeledImageDataset(root_dir=f'{base_dir}/Validation/image', transform=val_transforms)\n",
    "\n",
    "# Create DataLoader\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc270e-96f2-4111-a1e0-63fd092f28d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5443/1130257667.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
      "/tmp/ipykernel_5443/1130257667.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
      "/tmp/ipykernel_5443/1130257667.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n",
      "/tmp/ipykernel_5443/1130257667.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)  # Convert label to tensor\n"
     ]
    }
   ],
   "source": [
    "retrained_model = train_model(\n",
    "    model=model,\n",
    "    train_loader=augmented_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    is_initial=False,\n",
    "    val_loader=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd87e2-7c07-4de1-832d-93bf3386e44a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
