{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3145ac7-031f-427c-aa46-c49c7b2b96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1631c609-92b1-483c-b8e5-4382b2706138",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "279e5612-c75e-4ef1-a4e5-88d15335d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(image_dir):\n",
    "    # Create the SIFT object\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    all_keypoints = []  # To store keypoints for each image\n",
    "    all_descriptors = []  # To store descriptors for all images\n",
    "    image_names = []  # To store the names of images\n",
    "\n",
    "    # Iterate through each image in the input directory, convert to grayscale, and get the feature descriptors\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            continue\n",
    "            \n",
    "        keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "        \n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            all_keypoints.append(keypoints)  # Save keypoints with their corresponding file\n",
    "            all_descriptors.append(descriptors)\n",
    "            image_names.append(img_file)  # Store the image name separately\n",
    "\n",
    "    # Combine all descriptors into one array\n",
    "    combined_descriptors = np.vstack(all_descriptors) if all_descriptors else None\n",
    "\n",
    "    return image_names, all_keypoints, combined_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e07d651-9edd-4e2b-93a5-596f3e665ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(test_image_dir, dbscan_model, scaler):\n",
    "    anomalies = []\n",
    "\n",
    "    # Extract keypoints and descriptors from all test images\n",
    "    image_names, all_keypoints, combined_descriptors = extract_sift_features(test_image_dir)\n",
    "\n",
    "    # Ensure that keypoints and descriptors are aligned\n",
    "    for image_name, keypoints, descriptors in zip(image_names, all_keypoints, combined_descriptors):\n",
    "        image_path = os.path.join(test_image_dir, image_name)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Ensure descriptors are available for this image\n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            # Reshape descriptors if it's a single descriptor to ensure it's 2D\n",
    "            if descriptors.ndim == 1:\n",
    "                descriptors = descriptors.reshape(1, -1)\n",
    "\n",
    "            # Scale descriptors before applying DBSCAN\n",
    "            descriptors_scaled = scaler.transform(descriptors)\n",
    "\n",
    "            # Predict with the pre-trained DBSCAN model (on the \"good\" training images)\n",
    "            cluster_labels = dbscan_model.fit_predict(descriptors_scaled)\n",
    "\n",
    "            # If any descriptors are labeled as -1, they are considered anomalies\n",
    "            if -1 in cluster_labels:\n",
    "                anomalies.append(image_name)\n",
    "\n",
    "            # Visualize detected anomalies\n",
    "            for idx, kp in enumerate(keypoints):\n",
    "                if cluster_labels[idx] == -1:  # Only visualize anomalies\n",
    "                    x, y = int(kp.pt[0]), int(kp.pt[1])\n",
    "                    cv2.circle(image, (x, y), radius=5, color=(0, 0, 255), thickness=-1)  # Red circle for anomalies\n",
    "\n",
    "            # Save the image with \"_anomalies\" appended to the name\n",
    "            file_name, file_ext = os.path.splitext(image_name)  # Split the name and extension\n",
    "            output_image_name = f\"{file_name}_anomalies{file_ext}\"\n",
    "            output_image_path = os.path.join(test_image_dir, output_image_name)\n",
    "            cv2.imwrite(output_image_path, image)\n",
    "\n",
    "    return anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ea3be6-e1a3-4a8c-9085-deb6e10931ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_detection(image_path, ground_truth_path, anomaly_keypoints):\n",
    "    # Load ground truth mask where white indicates a region with a defect\n",
    "    ground_truth_mask = cv2.imread(ground_truth_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Create an anomaly map based on keypoints\n",
    "    anomaly_map = np.zeros_like(ground_truth_mask)\n",
    "    for kp in anomaly_keypoints:\n",
    "        x, y = int(kp.pt[0]), int(kp.pt[1])\n",
    "        cv2.circle(anomaly_map, (x, y), radius=5, color=1, thickness=-1)  # Mark anomalies\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    intersection = np.logical_and(ground_truth_mask, anomaly_map)\n",
    "    union = np.logical_or(ground_truth_mask, anomaly_map)\n",
    "    iou = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0\n",
    "\n",
    "    return {\"IoU\": iou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b7df2ee-1f9d-48bf-9c71-ab27ec2fe178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: capsule\n",
      "Extracted SIFT features from: capsule\n",
      "Train descriptors scaled successfully.\n",
      "Fit DBSCAN to scaled train image features\n",
      "Processing defect type: crack\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Process defect directories\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing defect type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefect_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m     anomalies \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdefect_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbscan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected anomalies: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manomalies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# Evaluate detected anomalies against ground truth masks\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# ground_truth_defect_path = os.path.join(ground_truth_path, defect_type)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# for anomaly in anomalies:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#         results = evaluate_anomaly_detection(test_image_path, gt_image_path, anomaly_keypoints=[])\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#         print(f\"Evaluation for {anomaly}: {results}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 30\u001b[0m, in \u001b[0;36mdetect_anomalies\u001b[0;34m(test_image_dir, dbscan_model, scaler)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Visualize detected anomalies\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, kp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keypoints):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcluster_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# Only visualize anomalies\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(kp\u001b[38;5;241m.\u001b[39mpt[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(kp\u001b[38;5;241m.\u001b[39mpt[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     32\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mcircle(image, (x, y), radius\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, color\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Red circle for anomalies\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# First approach is to use SIFT feature extraction and the DBSCAN clustering algorithm to detect anomalies. Will use the\n",
    "# images in the \"train/good\" directory to establish a baseline of features for defect-free images.\n",
    "\n",
    "items = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "# Iterate through the directories of images\n",
    "for item in items:\n",
    "    item_path = os.path.join(base_dir, item)\n",
    "    train_path = os.path.join(item_path, 'train/good')\n",
    "    test_path = os.path.join(item_path, 'test')\n",
    "    ground_truth_path = os.path.join(item_path, 'ground_truth')\n",
    "\n",
    "    print(f\"Processing: {item}\")\n",
    "\n",
    "    # Extract features from defect-free training images\n",
    "    names, keypoints, train_descriptors = extract_sift_features(train_path)\n",
    "    print(\"Extracted SIFT features from:\", item)\n",
    "\n",
    "    # Scale descriptors before clustering\n",
    "    if train_descriptors.size == 0:\n",
    "        print(\"No valid descriptors found in the training images.\")\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        train_features_scaled = scaler.fit_transform(train_descriptors)\n",
    "        print(\"Train descriptors scaled successfully.\")\n",
    "    \n",
    "    # Fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "    dbscan.fit(train_features_scaled)\n",
    "    print(\"Fit DBSCAN to scaled train image features\")\n",
    "\n",
    "    # Detect anomalies in test images. Test imageset also contains a \"good\" subdirectory, which can be used for detecting false-positives.\n",
    "    for defect_type in os.listdir(test_path):\n",
    "        defect_path = os.path.join(test_path, defect_type)\n",
    "\n",
    "        if defect_type == 'good':\n",
    "            print(\"Processing 'good' images for false positives.\")\n",
    "            anomalies = detect_anomalies(defect_path, dbscan, scaler)\n",
    "            print(f\"False Positives in 'good' images: {anomalies}\\n\")\n",
    "        else:\n",
    "            # Process defect directories\n",
    "            print(f\"Processing defect type: {defect_type}\")\n",
    "            anomalies = detect_anomalies(defect_path, dbscan, scaler)\n",
    "            print(f\"Detected anomalies: {anomalies}\\n\")\n",
    "\n",
    "            # Evaluate detected anomalies against ground truth masks\n",
    "            # ground_truth_defect_path = os.path.join(ground_truth_path, defect_type)\n",
    "            # for anomaly in anomalies:\n",
    "            #     test_image_path = os.path.join(defect_path, anomaly)\n",
    "                \n",
    "            #     base_name, ext = os.path.splitext(anomaly)\n",
    "            #     gt_image_name = f\"{base_name}_mask{ext}\"\n",
    "            #     gt_image_path = os.path.join(ground_truth_defect_path, gt_image_name)\n",
    "\n",
    "            #     if os.path.exists(gt_image_path):\n",
    "            #         results = evaluate_anomaly_detection(test_image_path, gt_image_path, anomaly_keypoints=[])\n",
    "            #         print(f\"Evaluation for {anomaly}: {results}\")\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eaab0-e3c8-44ba-bd44-4390d3b35572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
