{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3145ac7-031f-427c-aa46-c49c7b2b96d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1631c609-92b1-483c-b8e5-4382b2706138",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"Images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279e5612-c75e-4ef1-a4e5-88d15335d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sift_features(image_dir):\n",
    "    # Create the SIFT object\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # List will be used to hold the descriptors for all processed images\n",
    "    descriptors_list = []\n",
    "\n",
    "    # Iterate through each image in the input directory, covert to grayscale, and get the feature descriptors\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            continue\n",
    "        _, descriptors = sift.detectAndCompute(image, None)\n",
    "        if descriptors is not None:\n",
    "            descriptors_list.append(descriptors)\n",
    "\n",
    "    # Combine all descriptors into one array\n",
    "    return np.vstack(descriptors_list) if descriptors_list else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e07d651-9edd-4e2b-93a5-596f3e665ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(image_dir, dbscan_model, scaler):\n",
    "    sift = cv2.SIFT_create()\n",
    "    anomalies = []\n",
    "\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            continue\n",
    "        _, descriptors = sift.detectAndCompute(image, None)\n",
    "        if descriptors is None:\n",
    "            continue\n",
    "\n",
    "        # Scale descriptors\n",
    "        descriptors_scaled = scaler.transform(descriptors)\n",
    "        labels = dbscan_model.fit_predict(descriptors_scaled)\n",
    "\n",
    "        # Anomalous keypoints\n",
    "        if -1 in labels:\n",
    "            anomalies.append(img_file)\n",
    "\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ea3be6-e1a3-4a8c-9085-deb6e10931ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_detection(image_path, ground_truth_path, anomaly_keypoints):\n",
    "    # Load ground truth mask\n",
    "    ground_truth_mask = cv2.imread(ground_truth_path, cv2.IMREAD_GRAYSCALE)\n",
    "    ground_truth_mask = (ground_truth_mask > 0).astype(np.uint8)  # Binarize the mask\n",
    "\n",
    "    # Create an anomaly map based on keypoints\n",
    "    anomaly_map = np.zeros_like(ground_truth_mask)\n",
    "    for kp in anomaly_keypoints:\n",
    "        x, y = int(kp.pt[0]), int(kp.pt[1])\n",
    "        cv2.circle(anomaly_map, (x, y), radius=5, color=1, thickness=-1)  # Mark anomalies\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    intersection = np.logical_and(ground_truth_mask, anomaly_map)\n",
    "    union = np.logical_or(ground_truth_mask, anomaly_map)\n",
    "    iou = np.sum(intersection) / np.sum(union) if np.sum(union) > 0 else 0\n",
    "\n",
    "    return {\"IoU\": iou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b7df2ee-1f9d-48bf-9c71-ab27ec2fe178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: capsule\n",
      "Extracted SIFT features from: capsule\n",
      "Fit DBSCAN to scaled train image features\n",
      "Processing defect type: crack\n",
      "Detected anomalies: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '021.png', '018.png', '007.png', '010.png', '005.png', '022.png', '004.png', '009.png', '003.png', '020.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Evaluation for 000.png: {'IoU': 0.0}\n",
      "Evaluation for 019.png: {'IoU': 0.0}\n",
      "Evaluation for 008.png: {'IoU': 0.0}\n",
      "Evaluation for 013.png: {'IoU': 0.0}\n",
      "Evaluation for 017.png: {'IoU': 0.0}\n",
      "Evaluation for 002.png: {'IoU': 0.0}\n",
      "Evaluation for 021.png: {'IoU': 0.0}\n",
      "Evaluation for 018.png: {'IoU': 0.0}\n",
      "Evaluation for 007.png: {'IoU': 0.0}\n",
      "Evaluation for 010.png: {'IoU': 0.0}\n",
      "Evaluation for 005.png: {'IoU': 0.0}\n",
      "Evaluation for 022.png: {'IoU': 0.0}\n",
      "Evaluation for 004.png: {'IoU': 0.0}\n",
      "Evaluation for 009.png: {'IoU': 0.0}\n",
      "Evaluation for 003.png: {'IoU': 0.0}\n",
      "Evaluation for 020.png: {'IoU': 0.0}\n",
      "Evaluation for 011.png: {'IoU': 0.0}\n",
      "Evaluation for 015.png: {'IoU': 0.0}\n",
      "Evaluation for 016.png: {'IoU': 0.0}\n",
      "Evaluation for 012.png: {'IoU': 0.0}\n",
      "Evaluation for 006.png: {'IoU': 0.0}\n",
      "Evaluation for 001.png: {'IoU': 0.0}\n",
      "Evaluation for 014.png: {'IoU': 0.0}\n",
      "Processing defect type: poke\n",
      "Detected anomalies: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '018.png', '007.png', '010.png', '005.png', '004.png', '009.png', '003.png', '020.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Evaluation for 000.png: {'IoU': 0.0}\n",
      "Evaluation for 019.png: {'IoU': 0.0}\n",
      "Evaluation for 008.png: {'IoU': 0.0}\n",
      "Evaluation for 013.png: {'IoU': 0.0}\n",
      "Evaluation for 017.png: {'IoU': 0.0}\n",
      "Evaluation for 002.png: {'IoU': 0.0}\n",
      "Evaluation for 018.png: {'IoU': 0.0}\n",
      "Evaluation for 007.png: {'IoU': 0.0}\n",
      "Evaluation for 010.png: {'IoU': 0.0}\n",
      "Evaluation for 005.png: {'IoU': 0.0}\n",
      "Evaluation for 004.png: {'IoU': 0.0}\n",
      "Evaluation for 009.png: {'IoU': 0.0}\n",
      "Evaluation for 003.png: {'IoU': 0.0}\n",
      "Evaluation for 020.png: {'IoU': 0.0}\n",
      "Evaluation for 011.png: {'IoU': 0.0}\n",
      "Evaluation for 015.png: {'IoU': 0.0}\n",
      "Evaluation for 016.png: {'IoU': 0.0}\n",
      "Evaluation for 012.png: {'IoU': 0.0}\n",
      "Evaluation for 006.png: {'IoU': 0.0}\n",
      "Evaluation for 001.png: {'IoU': 0.0}\n",
      "Evaluation for 014.png: {'IoU': 0.0}\n",
      "Processing 'good' images for false positives.\n",
      "False Positives in 'good' images: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '021.png', '018.png', '007.png', '010.png', '005.png', '022.png', '004.png', '009.png', '003.png', '020.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Processing defect type: faulty_imprint\n",
      "Detected anomalies: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '021.png', '018.png', '007.png', '010.png', '005.png', '004.png', '009.png', '003.png', '020.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Evaluation for 000.png: {'IoU': 0.0}\n",
      "Evaluation for 019.png: {'IoU': 0.0}\n",
      "Evaluation for 008.png: {'IoU': 0.0}\n",
      "Evaluation for 013.png: {'IoU': 0.0}\n",
      "Evaluation for 017.png: {'IoU': 0.0}\n",
      "Evaluation for 002.png: {'IoU': 0.0}\n",
      "Evaluation for 021.png: {'IoU': 0.0}\n",
      "Evaluation for 018.png: {'IoU': 0.0}\n",
      "Evaluation for 007.png: {'IoU': 0.0}\n",
      "Evaluation for 010.png: {'IoU': 0.0}\n",
      "Evaluation for 005.png: {'IoU': 0.0}\n",
      "Evaluation for 004.png: {'IoU': 0.0}\n",
      "Evaluation for 009.png: {'IoU': 0.0}\n",
      "Evaluation for 003.png: {'IoU': 0.0}\n",
      "Evaluation for 020.png: {'IoU': 0.0}\n",
      "Evaluation for 011.png: {'IoU': 0.0}\n",
      "Evaluation for 015.png: {'IoU': 0.0}\n",
      "Evaluation for 016.png: {'IoU': 0.0}\n",
      "Evaluation for 012.png: {'IoU': 0.0}\n",
      "Evaluation for 006.png: {'IoU': 0.0}\n",
      "Evaluation for 001.png: {'IoU': 0.0}\n",
      "Evaluation for 014.png: {'IoU': 0.0}\n",
      "Processing defect type: scratch\n",
      "Detected anomalies: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '021.png', '018.png', '007.png', '010.png', '005.png', '022.png', '004.png', '009.png', '003.png', '020.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Evaluation for 000.png: {'IoU': 0.0}\n",
      "Evaluation for 019.png: {'IoU': 0.0}\n",
      "Evaluation for 008.png: {'IoU': 0.0}\n",
      "Evaluation for 013.png: {'IoU': 0.0}\n",
      "Evaluation for 017.png: {'IoU': 0.0}\n",
      "Evaluation for 002.png: {'IoU': 0.0}\n",
      "Evaluation for 021.png: {'IoU': 0.0}\n",
      "Evaluation for 018.png: {'IoU': 0.0}\n",
      "Evaluation for 007.png: {'IoU': 0.0}\n",
      "Evaluation for 010.png: {'IoU': 0.0}\n",
      "Evaluation for 005.png: {'IoU': 0.0}\n",
      "Evaluation for 022.png: {'IoU': 0.0}\n",
      "Evaluation for 004.png: {'IoU': 0.0}\n",
      "Evaluation for 009.png: {'IoU': 0.0}\n",
      "Evaluation for 003.png: {'IoU': 0.0}\n",
      "Evaluation for 020.png: {'IoU': 0.0}\n",
      "Evaluation for 011.png: {'IoU': 0.0}\n",
      "Evaluation for 015.png: {'IoU': 0.0}\n",
      "Evaluation for 016.png: {'IoU': 0.0}\n",
      "Evaluation for 012.png: {'IoU': 0.0}\n",
      "Evaluation for 006.png: {'IoU': 0.0}\n",
      "Evaluation for 001.png: {'IoU': 0.0}\n",
      "Evaluation for 014.png: {'IoU': 0.0}\n",
      "Processing defect type: squeeze\n",
      "Detected anomalies: ['000.png', '019.png', '008.png', '013.png', '017.png', '002.png', '018.png', '007.png', '010.png', '005.png', '004.png', '009.png', '003.png', '011.png', '015.png', '016.png', '012.png', '006.png', '001.png', '014.png']\n",
      "\n",
      "Evaluation for 000.png: {'IoU': 0.0}\n",
      "Evaluation for 019.png: {'IoU': 0.0}\n",
      "Evaluation for 008.png: {'IoU': 0.0}\n",
      "Evaluation for 013.png: {'IoU': 0.0}\n",
      "Evaluation for 017.png: {'IoU': 0.0}\n",
      "Evaluation for 002.png: {'IoU': 0.0}\n",
      "Evaluation for 018.png: {'IoU': 0.0}\n",
      "Evaluation for 007.png: {'IoU': 0.0}\n",
      "Evaluation for 010.png: {'IoU': 0.0}\n",
      "Evaluation for 005.png: {'IoU': 0.0}\n",
      "Evaluation for 004.png: {'IoU': 0.0}\n",
      "Evaluation for 009.png: {'IoU': 0.0}\n",
      "Evaluation for 003.png: {'IoU': 0.0}\n",
      "Evaluation for 011.png: {'IoU': 0.0}\n",
      "Evaluation for 015.png: {'IoU': 0.0}\n",
      "Evaluation for 016.png: {'IoU': 0.0}\n",
      "Evaluation for 012.png: {'IoU': 0.0}\n",
      "Evaluation for 006.png: {'IoU': 0.0}\n",
      "Evaluation for 001.png: {'IoU': 0.0}\n",
      "Evaluation for 014.png: {'IoU': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# First approach is to use SIFT feature extraction and the DBSCAN clustering algorithm to detect anomalies. Will use the\n",
    "# images in the \"train/good\" directory to establish a baseline of features for defect-free images.\n",
    "\n",
    "items = [f for f in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, f))]\n",
    "# Iterate through the directories of images\n",
    "for item in items:\n",
    "    item_path = os.path.join(base_dir, item)\n",
    "    train_path = os.path.join(item_path, 'train/good')\n",
    "    test_path = os.path.join(item_path, 'test')\n",
    "    ground_truth_path = os.path.join(item_path, 'ground_truth')\n",
    "\n",
    "    print(f\"Processing: {item}\")\n",
    "\n",
    "    # Extract features from defect-free training images\n",
    "    train_features = extract_sift_features(train_path)\n",
    "    print(\"Extracted SIFT features from:\", item)\n",
    "\n",
    "    # Scale features before giving to clustering algorithm\n",
    "    scaler = StandardScaler()\n",
    "    train_features_scaled = scaler.fit_transform(train_features)\n",
    "    \n",
    "    # Fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5, metric='euclidean')\n",
    "    dbscan.fit(train_features_scaled)\n",
    "    print(\"Fit DBSCAN to scaled train image features\")\n",
    "\n",
    "    # Detect anomalies in test images. Test imageset also contains a \"good\" subdirectory, which can be used for detecting false-positives.\n",
    "    for defect_type in os.listdir(test_path):\n",
    "        defect_path = os.path.join(test_path, defect_type)\n",
    "\n",
    "        if defect_type == 'good':\n",
    "            print(\"Processing 'good' images for false positives.\")\n",
    "            anomalies = detect_anomalies(defect_path, dbscan, scaler)\n",
    "            print(f\"False Positives in 'good' images: {anomalies}\\n\")\n",
    "        else:\n",
    "            # Process defect directories\n",
    "            print(f\"Processing defect type: {defect_type}\")\n",
    "            anomalies = detect_anomalies(defect_path, dbscan, scaler)\n",
    "            print(f\"Detected anomalies: {anomalies}\\n\")\n",
    "\n",
    "            # Evaluate detected anomalies against ground truth masks\n",
    "            ground_truth_defect_path = os.path.join(ground_truth_path, defect_type)\n",
    "            for anomaly in anomalies:\n",
    "                test_image_path = os.path.join(defect_path, anomaly)\n",
    "                \n",
    "                base_name, ext = os.path.splitext(anomaly)\n",
    "                gt_image_name = f\"{base_name}_mask{ext}\"\n",
    "                gt_image_path = os.path.join(ground_truth_defect_path, gt_image_name)\n",
    "\n",
    "                if os.path.exists(gt_image_path):\n",
    "                    results = evaluate_anomaly_detection(test_image_path, gt_image_path, anomaly_keypoints=[])\n",
    "                    print(f\"Evaluation for {anomaly}: {results}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5eaab0-e3c8-44ba-bd44-4390d3b35572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
