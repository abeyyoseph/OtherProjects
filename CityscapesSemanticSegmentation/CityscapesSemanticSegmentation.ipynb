{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d291d29-0116-4f7b-88c2-1022a7dc3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fc5bc15-bf37-4605-8922-9375671dd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"CityscapesDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b039e416-2c06-4a8e-8d40-5e5d8c67432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code is taken from: https://github.molgen.mpg.de/mohomran/cityscapes/blob/master/scripts/helpers/labels.py#L55\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "\n",
    "    'trainId'     , # An integer ID that overwrites the ID above, when creating ground truth\n",
    "                    # images for training.\n",
    "                    # For training, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# A list of all labels\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please adapt the train IDs as appropriate for you approach.\n",
    "# Note that you might want to ignore labels with ID 255 during training.\n",
    "# Make sure to provide your results using the original IDs and not the training IDs.\n",
    "# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'ground'          , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'ground'          , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'ground'          , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'ground'          , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c22d9d86-1a2a-4e8d-b539-51daf411c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create color to trainid mapping\n",
    "COLOR_TO_TRAINID = {label.color: label.trainId for label in labels if label.trainId != 255}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50b4a093-3c51-4613-860f-9039ddf366a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_labels(mask, mapping):\n",
    "    \n",
    "    closest_distance = np.full([mask.shape[0], mask.shape[1]], np.inf) # Large initial distance\n",
    "    closest_category = np.full([mask.shape[0], mask.shape[1]], 255, dtype=np.int32) # Initialize to unlabeld category 255  \n",
    "\n",
    "    # Iterate over every color mapping\n",
    "    for color, train_id in mapping.items():\n",
    "        # Ensure color is a NumPy array\n",
    "        color = np.array(color, dtype=np.int32)\n",
    "\n",
    "        # Compute Euclidean distance between the mask and the current color in the mapping\n",
    "        dist = np.linalg.norm(mask - color.reshape(1, 1, 3), axis=-1)\n",
    "\n",
    "        # Find where the current color is the closest so far\n",
    "        is_closer = dist < closest_distance\n",
    "\n",
    "        # Only update where `is_closer` is True\n",
    "        closest_distance[is_closer] = dist[is_closer]\n",
    "        closest_category[is_closer] = train_id  # Assign train_id value\n",
    "\n",
    "    return closest_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09875399-4028-4f6c-97c3-cba7ac06306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Paths for images and labels\n",
    "        self.img_dir = os.path.join(root_dir, split, \"img\")\n",
    "        self.mask_dir = os.path.join(root_dir, split, \"label\")\n",
    "\n",
    "        # List of image files\n",
    "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
    "        self.mask_filenames = sorted(os.listdir(self.mask_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "\n",
    "        # Load image and label\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"RGB\")\n",
    "\n",
    "        # Convert mask to class labels using the color mapping\n",
    "        mask = find_closest_labels(np.array(mask), COLOR_TO_TRAINID)\n",
    "        # Replace any unknown pixels with 255\n",
    "        mask[mask == -1] = 255\n",
    "        \n",
    "        mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image) # Ensure image is a tensor\n",
    "            \n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc419dd-0932-4d8a-ada3-aec2303172d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dice loss metric\n",
    "def dice_loss(pred, target, epsilon=1e-6):\n",
    "    # Apply softmax to get probabilities for each class from the logits\n",
    "    pred = F.softmax(pred, dim=1)\n",
    "\n",
    "    # One-hot encode the target\n",
    "    target_one_hot = F.one_hot(target.long(), num_classes=pred.shape[1])\n",
    "    target_one_hot = target_one_hot.permute(0, 3, 1, 2).float()\n",
    "\n",
    "    # Compute the intersection and union\n",
    "    intersection = torch.sum(pred_flat * target_flat, dim=2)\n",
    "    union = torch.sum(pred_flat, dim=2) + torch.sum(target_flat, dim=2)\n",
    "\n",
    "    # Dice coefficient and loss\n",
    "    dice = (2.0 * intersection + epsilon) / (union + epsilon)\n",
    "    return 1 - dice.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29cb1f0-7af1-4ebc-913c-3765a858f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations. Since the images are pretty small (256x96), random cropping would remove\n",
    "# too much valuable information. If blurring the image, a smaller kernel should be applied. Color jitter\n",
    "# should be applied conservatively to avoid excessive distortion. Scaling also should be limited since \n",
    "# the images are small.\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96), interpolation=transforms.InterpolationMode.NEAREST)  \n",
    "])\n",
    "\n",
    "val_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29e450a0-6f88-4eb4-a158-c85c1bcba825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CityscapesDataset objects for the training and validation datasets\n",
    "train_dataset = CityscapesDataset(\n",
    "    root_dir=base_dir,\n",
    "    split=\"train\",\n",
    "    transform=train_image_transform,\n",
    "    target_transform=train_label_transform)\n",
    "\n",
    "val_dataset = CityscapesDataset(\n",
    "    root_dir=base_dir,\n",
    "    split=\"val\",\n",
    "    transform=val_image_transform,\n",
    "    target_transform=val_label_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab631889-466d-49b7-98b4-f5ab0c1b8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DeepLabV3 model with the ResNet50 backbone\n",
    "model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "469cfa0c-5059-4964-8cf4-e2abdad25867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abey/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Modify the model's final layer for the Cityscapes dataset\n",
    "num_classes = len(labels)\n",
    "model.classifier[4] = torch.nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b76a54e9-c839-4eba-b57f-203cb0ff3670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning rate scheduler with warmup\n",
    "def get_linear_warmup_scheduler(optimizer, base_lr, num_epochs, num_warmup=3):\n",
    "    def lr_lambda(epoch):\n",
    "        return min(1.0, (epoch+1) / num_warmup)\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d96c8508-9530-4945-a97f-86f623d857ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255) # pixels with 255 not included in loss\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "decay = 0.001\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=decay)\n",
    "num_epochs = 10\n",
    "scheduler = get_linear_warmup_scheduler(optimizer, learning_rate, num_epochs)\n",
    "\n",
    "best_model_path = \"best_model.pth\" # Path to save the best model based on validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51af685d-8af4-49a7-9ee6-53cbd2f2e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the training and validation datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2fcc4ec-0e26-4c0f-9c08-07aea740fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs, optimizer, scheduler):\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        print(f\"Epoch {epoch+1}: LR = {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)['out'] # DeepLabV3 outputs a dict with 'out'\n",
    "            \n",
    "            loss = criterion(outputs, masks.squeeze(1).long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Switch to eval for validation\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)['out']\n",
    "                loss = criterion(outputs, masks.squeeze(1).long())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if the validation loss has improved\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "725d2cb0-37e9-4208-97b2-fbb7ce872c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: LR = 0.000333\n",
      "Epoch [1/10], Train Loss: 1.1057\n",
      "Validation Loss: 1.0012\n",
      "New best model saved with validation loss: 1.0012\n",
      "Epoch 2: LR = 0.000667\n",
      "Epoch [2/10], Train Loss: 1.0808\n",
      "Validation Loss: 1.0774\n",
      "Epoch 3: LR = 0.001000\n",
      "Epoch [3/10], Train Loss: 1.0460\n",
      "Validation Loss: 1.0414\n",
      "Epoch 4: LR = 0.001000\n",
      "Epoch [4/10], Train Loss: 0.9794\n",
      "Validation Loss: 1.0751\n",
      "Epoch 5: LR = 0.001000\n",
      "Epoch [5/10], Train Loss: 0.9239\n",
      "Validation Loss: 1.3617\n",
      "Epoch 6: LR = 0.001000\n",
      "Epoch [6/10], Train Loss: 0.8884\n",
      "Validation Loss: 1.0338\n",
      "Epoch 7: LR = 0.001000\n",
      "Epoch [7/10], Train Loss: 0.8675\n",
      "Validation Loss: 0.8378\n",
      "New best model saved with validation loss: 0.8378\n",
      "Epoch 8: LR = 0.001000\n",
      "Epoch [8/10], Train Loss: 0.8330\n",
      "Validation Loss: 1.0182\n",
      "Epoch 9: LR = 0.001000\n",
      "Epoch [9/10], Train Loss: 0.8189\n",
      "Validation Loss: 1.2605\n",
      "Epoch 10: LR = 0.001000\n",
      "Epoch [10/10], Train Loss: 0.8037\n",
      "Validation Loss: 0.8466\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, num_epochs, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7fd7371-19a2-4cab-9043-edf504db4bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([32, 3, 256, 96])\n",
      "Mask shape: torch.Size([32, 1, 256, 96])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    image = batch[0]\n",
    "    mask = batch[1]\n",
    "\n",
    "    print(f\"Image shape: {image.shape}\")  # Should be (3, H, W)\n",
    "    print(f\"Mask shape: {mask.shape}\")    # Should be (1, H, W)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6899124-68e3-4d61-a669-ad3d4f74b1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
