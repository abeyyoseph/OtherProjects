{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d291d29-0116-4f7b-88c2-1022a7dc3290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc5bc15-bf37-4605-8922-9375671dd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"CityscapesDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09875399-4028-4f6c-97c3-cba7ac06306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom dataset class\n",
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        # Paths for images and labels\n",
    "        self.img_dir = os.path.join(root_dir, split, \"img\")\n",
    "        self.label_dir = os.path.join(root_dir, split, \"label\")\n",
    "\n",
    "        # List of image files\n",
    "        self.img_filenames = sorted(os.listdir(self.img_dir))\n",
    "        self.label_filenames = sorted(os.listdir(self.label_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_filenames[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_filenames[idx])\n",
    "\n",
    "        # Load image and label\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = Image.open(label_path)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a29cb1f0-7af1-4ebc-913c-3765a858f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations. Since the images are pretty small (256x96), random cropping would remove\n",
    "# too much valuable information. If blurring the image, a smaller kernel should be applied. Color jitter\n",
    "# should be applied conservatively to avoid excessive distortion. Scaling also should be limited since \n",
    "# the images are small.\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # Color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Ensure masks get the same flip\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "val_image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_label_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 96)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e450a0-6f88-4eb4-a158-c85c1bcba825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CityscapesDataset objects for the training and validation datasets\n",
    "train_dataset = CityscapesDataset(\n",
    "    root_dir=base_dir,\n",
    "    split=\"train\",\n",
    "    transform=train_image_transform,\n",
    "    target_transform=train_label_transform)\n",
    "\n",
    "val_dataset = CityscapesDataset(\n",
    "    root_dir=base_dir,\n",
    "    split=\"val\",\n",
    "    transform=val_image_transform,\n",
    "    target_transform=val_label_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d2d36f-9754-4114-a85b-9d324c450670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for the training and validation datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab631889-466d-49b7-98b4-f5ab0c1b8a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /home/abey/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n",
      "100%|████████████████████████████████████████| 161M/161M [00:29<00:00, 5.63MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the DeepLabV3 model with the ResNet50 backbone\n",
    "model = models.deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cfa0c-5059-4964-8cf4-e2abdad25867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
